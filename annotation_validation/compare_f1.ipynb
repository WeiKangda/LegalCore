{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!git clone https://github.com/boberle/corefconversion"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "num = \"55\" #enter the number of the document"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the extension to .sarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "source": [
    "!cp /Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/{num}.txt /Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/{num}_ishaan.sarc"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "source": [
    "!cp /Users/ishaan/Documents/ruihong_lab/legal_project/jonathan_annotations/only_long_dist/{num}.txt /Users/ishaan/Documents/ruihong_lab/legal_project/jonathan_annotations/only_long_dist/{num}_jonathan.sarc"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert from .sarc to BRAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "source": [
    "#convert from sarc to brat format\n",
    "!python3 ./corefconversion/sacr2ann.py /Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/{num}_ishaan.sarc --txt /Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/{num}_ishaan.txt --ann /Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/{num}_ishaan.ann"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "source": [
    "#convert from sarc to brat format\n",
    "!python3 ./corefconversion/sacr2ann.py /Users/ishaan/Documents/ruihong_lab/legal_project/jonathan_annotations/only_long_dist/{num}_jonathan.sarc --txt /Users/ishaan/Documents/ruihong_lab/legal_project/jonathan_annotations/only_long_dist/{num}_jonathan.txt --ann /Users/ishaan/Documents/ruihong_lab/legal_project/jonathan_annotations/only_long_dist/{num}_jonathan.ann"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#note the cluster_a is predicted and cluster_b is ground truth cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "from itertools import combinations"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "source": [
    "def parse_sarc_file(filename):\n",
    "    mentions = {}\n",
    "    coreferences = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"T\"):  # Mention\n",
    "                parts = line.strip().split('\\t')\n",
    "                mention_id = parts[0]\n",
    "                mention_text = parts[2]\n",
    "                # Capture position as a unique identifier for each mention\n",
    "                mention_pos = tuple(map(int, parts[1].split()[1:3]))\n",
    "                mentions[mention_id] = (mention_text, mention_pos)\n",
    "                \n",
    "            elif line.startswith(\"R\"):  # Coreference link\n",
    "                match = re.match(r\"R\\d+\\s+Coreference\\s+Arg1:(T\\d+)\\s+Arg2:(T\\d+)\", line)\n",
    "                if match:\n",
    "                    arg1, arg2 = match.groups()\n",
    "                    coreferences.append((arg1, arg2))\n",
    "    \n",
    "    return mentions, coreferences\n",
    "\n",
    "def build_clusters(mentions, coref_links):\n",
    "    clusters = defaultdict(list)\n",
    "    \n",
    "    mention_to_cluster = {}\n",
    "    cluster_id = 0\n",
    "    \n",
    "    for arg1, arg2 in coref_links:\n",
    "        if arg1 not in mention_to_cluster and arg2 not in mention_to_cluster:\n",
    "            # New cluster\n",
    "            mention_to_cluster[arg1] = cluster_id\n",
    "            mention_to_cluster[arg2] = cluster_id\n",
    "            clusters[cluster_id].extend([mentions[arg1], mentions[arg2]])\n",
    "            cluster_id += 1\n",
    "        elif arg1 in mention_to_cluster:\n",
    "            cluster_idx = mention_to_cluster[arg1]\n",
    "            clusters[cluster_idx].append(mentions[arg2])\n",
    "            mention_to_cluster[arg2] = cluster_idx\n",
    "        elif arg2 in mention_to_cluster:\n",
    "            cluster_idx = mention_to_cluster[arg2]\n",
    "            clusters[cluster_idx].append(mentions[arg1])\n",
    "            mention_to_cluster[arg1] = cluster_idx\n",
    "            \n",
    "    return list(clusters.values())\n",
    "\n",
    "# Convert clusters to sets of (name, position) tuples for comparison\n",
    "def clusters_to_sets(clusters):\n",
    "    return [set(cluster) for cluster in clusters]\n",
    "\n",
    "def calculate_precision_recall(predicted_clusters, ground_truth_clusters):\n",
    "    predicted_sets = clusters_to_sets(predicted_clusters)\n",
    "    ground_truth_sets = clusters_to_sets(ground_truth_clusters)\n",
    "    \n",
    "    correct = sum(1 for pc in predicted_sets if pc in ground_truth_sets)\n",
    "    precision = correct / len(predicted_sets) if predicted_sets else 0\n",
    "    recall = correct / len(ground_truth_sets) if ground_truth_sets else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Process the files and evaluate\n",
    "# mentions_b, coref_links_b = parse_sarc_file('/Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/90_ishaan.ann')\n",
    "mentions_a, coref_links_a = parse_sarc_file(f'/Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/{num}_ishaan.ann')\n",
    "mentions_b, coref_links_b = parse_sarc_file(f'/Users/ishaan/Documents/ruihong_lab/legal_project/jonathan_annotations/only_long_dist/{num}_jonathan.ann')\n",
    "\n",
    "\n",
    "clusters_a = build_clusters(mentions_a, coref_links_a)\n",
    "clusters_b = build_clusters(mentions_b, coref_links_b)\n",
    "\n",
    "precision_a, recall_a, f1_a = calculate_precision_recall(clusters_a, clusters_b)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "source": [
    "def muc_score(clusters_a, clusters_b):\n",
    "    \"\"\"\n",
    "    Calculate MUC score for coreference resolution.\n",
    "    \n",
    "    Args:\n",
    "        clusters_a, clusters_b: Lists of clusters, where each cluster contains tuples of (mention_text, position)\n",
    "    \"\"\"\n",
    "    def get_mention_pairs(cluster):\n",
    "        \"\"\"Convert a cluster into a set of mention pairs based on positions\"\"\"\n",
    "        mentions = sorted(cluster, key=lambda x: x[1])  # Sort by position\n",
    "        return set(combinations(mentions, 2))\n",
    "    \n",
    "    # Convert clusters to sets and get all mention pairs\n",
    "    clusters_a = [set(cluster) for cluster in clusters_a]\n",
    "    clusters_b = [set(cluster) for cluster in clusters_b]\n",
    "    \n",
    "    # Create mention to cluster mappings\n",
    "    mention_to_cluster_a = {}\n",
    "    mention_to_cluster_b = {}\n",
    "    \n",
    "    for i, cluster in enumerate(clusters_a):\n",
    "        for mention in cluster:\n",
    "            mention_to_cluster_a[mention] = i\n",
    "            \n",
    "    for i, cluster in enumerate(clusters_b):\n",
    "        for mention in cluster:\n",
    "            mention_to_cluster_b[mention] = i\n",
    "    \n",
    "    # Calculate links for each cluster\n",
    "    links_a = set()\n",
    "    links_b = set()\n",
    "    \n",
    "    for cluster in clusters_a:\n",
    "        links_a.update(get_mention_pairs(cluster))\n",
    "        \n",
    "    for cluster in clusters_b:\n",
    "        links_b.update(get_mention_pairs(cluster))\n",
    "    \n",
    "    # Calculate correct links (intersection)\n",
    "    correct_links = links_a.intersection(links_b)\n",
    "    \n",
    "    # Calculate MUC scores\n",
    "    precision = len(correct_links) / len(links_a) if links_a else 0\n",
    "    recall = len(correct_links) / len(links_b) if links_b else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "def b3_score(predicted_clusters, ground_truth_clusters):\n",
    "    \"\"\"\n",
    "    Calculate B続 score for coreference resolution.\n",
    "    \n",
    "    Args:\n",
    "        predicted_clusters: List of clusters, where each cluster contains tuples of (mention_text, position)\n",
    "        ground_truth_clusters: List of clusters, where each cluster contains tuples of (mention_text, position)\n",
    "    \"\"\"\n",
    "    def get_mention_to_cluster_mapping(clusters):\n",
    "        mapping = {}\n",
    "        # Convert each cluster to a set if it isn't already\n",
    "        clusters = [set(cluster) for cluster in clusters]\n",
    "        for cluster in clusters:\n",
    "            for mention in cluster:\n",
    "                mapping[mention] = cluster\n",
    "        return mapping\n",
    "\n",
    "    # Create mappings\n",
    "    pred_mapping = get_mention_to_cluster_mapping(predicted_clusters)\n",
    "    gt_mapping = get_mention_to_cluster_mapping(ground_truth_clusters)\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    \n",
    "    # Use the union of all mentions to ensure we don't miss any\n",
    "    all_mentions = set(pred_mapping.keys()).union(set(gt_mapping.keys()))\n",
    "    num_mentions = len(all_mentions)\n",
    "    \n",
    "    for mention in all_mentions:\n",
    "        if mention in pred_mapping and mention in gt_mapping:\n",
    "            predicted_cluster = pred_mapping[mention]\n",
    "            ground_truth_cluster = gt_mapping[mention]\n",
    "            \n",
    "            intersection_size = len(predicted_cluster.intersection(ground_truth_cluster))\n",
    "            precision_for_mention = intersection_size / len(predicted_cluster)\n",
    "            recall_for_mention = intersection_size / len(ground_truth_cluster)\n",
    "            \n",
    "            total_precision += precision_for_mention\n",
    "            total_recall += recall_for_mention\n",
    "        elif mention in pred_mapping:\n",
    "            # Mention in predicted but not in ground truth\n",
    "            total_precision += 0\n",
    "        elif mention in gt_mapping:\n",
    "            # Mention in ground truth but not in predicted\n",
    "            total_recall += 0\n",
    "\n",
    "    precision = total_precision / num_mentions if num_mentions > 0 else 0\n",
    "    recall = total_recall / num_mentions if num_mentions > 0 else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Example usage:\n",
    "def evaluate_coreference(predicted_clusters, ground_truth_clusters):\n",
    "    \"\"\"\n",
    "    Evaluate using both MUC and B続 metrics\n",
    "    \"\"\"\n",
    "    # B続 scores\n",
    "    b3_p, b3_r, b3_f1 = b3_score(predicted_clusters, ground_truth_clusters)\n",
    "    print(\"\\nB続 Scores:\")\n",
    "    print(f\"Precision: {b3_p:.3f}\")\n",
    "    print(f\"Recall: {b3_r:.3f}\")\n",
    "    print(f\"F1: {b3_f1:.3f}\")\n",
    "    \n",
    "    # MUC scores\n",
    "    muc_p, muc_r, muc_f1 = muc_score(predicted_clusters, ground_truth_clusters)\n",
    "    print(\"\\nMUC Scores:\")\n",
    "    print(f\"Precision: {muc_p:.3f}\")\n",
    "    print(f\"Recall: {muc_r:.3f}\")\n",
    "    print(f\"F1: {muc_f1:.3f}\")\n",
    "\n",
    "     # Basic precision/recall\n",
    "    precision, recall, f1 = calculate_precision_recall(clusters_a, clusters_b)\n",
    "    print(\"\\nBasic Evaluation:\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1: {f1:.3f}\")\n",
    "    \n",
    "\n",
    "# Usage\n",
    "evaluate_coreference(clusters_a, clusters_b)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "source": [
    "def long_distance_evaluation(annotations_file_a, annotations_file_b, text_doc):\n",
    "    # Parse both annotation files\n",
    "    mentions_a, coref_links_a = parse_sarc_file(annotations_file_a)\n",
    "    mentions_b, coref_links_b = parse_sarc_file(annotations_file_b)\n",
    "\n",
    "    # Parse paragraphs\n",
    "    with open(text_doc, 'r') as file:\n",
    "        text = file.read()\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    def get_paragraph_index(position):\n",
    "        char_count = 0\n",
    "        for idx, para in enumerate(paragraphs):\n",
    "            char_count += len(para) + 2  # +2 for '\\n\\n'\n",
    "            if position <= char_count:\n",
    "                return idx\n",
    "        return len(paragraphs) - 1\n",
    "    \n",
    "    # Filter long-distance links for both sets\n",
    "    def filter_long_distance(coref_links, mentions):\n",
    "        long_distance_links = []\n",
    "        for arg1, arg2 in coref_links:\n",
    "            pos1 = mentions[arg1][1][0]  # Get start position of first mention\n",
    "            pos2 = mentions[arg2][1][0]  # Get start position of second mention\n",
    "            \n",
    "            para1 = get_paragraph_index(pos1)\n",
    "            para2 = get_paragraph_index(pos2)\n",
    "            \n",
    "            if para1 != para2:\n",
    "                long_distance_links.append((arg1, arg2))\n",
    "                \n",
    "        return long_distance_links\n",
    "\n",
    "    coref_links_a_long = filter_long_distance(coref_links_a, mentions_a)\n",
    "    coref_links_b_long = filter_long_distance(coref_links_b, mentions_b)\n",
    "    \n",
    "    # Build clusters using filtered long-distance coreferences\n",
    "    clusters_a = build_clusters(mentions_a, coref_links_a_long)\n",
    "    clusters_b = build_clusters(mentions_b, coref_links_b_long)\n",
    "\n",
    "    print(clusters_a)\n",
    "    print(clusters_b)\n",
    "\n",
    "    def get_long_distance_pairs(cluster):\n",
    "        \"\"\"Convert a cluster into a set of long-distance mention pairs\"\"\"\n",
    "        mentions = sorted(cluster, key=lambda x: x[1])\n",
    "        # pairs = set()\n",
    "        # for m1, m2 in combinations(mentions, 2):\n",
    "        #     pos1 = m1[1][0]\n",
    "        #     pos2 = m2[1][0]\n",
    "        #     para1 = get_paragraph_index(pos1)\n",
    "        #     para2 = get_paragraph_index(pos2)\n",
    "        #     if para1 != para2:\n",
    "        #         pairs.add((m1, m2))\n",
    "        # return pairs\n",
    "        return set(combinations(mentions, 2))\n",
    "\n",
    "    def long_distance_muc(clusters_a, clusters_b):\n",
    "        # Convert clusters to sets\n",
    "        clusters_a = [set(cluster) for cluster in clusters_a]\n",
    "        clusters_b = [set(cluster) for cluster in clusters_b]\n",
    "        \n",
    "        # Get all long-distance mention pairs\n",
    "        links_a = set()\n",
    "        links_b = set()\n",
    "        \n",
    "        for cluster in clusters_a:\n",
    "            links_a.update(get_long_distance_pairs(cluster))\n",
    "            \n",
    "        for cluster in clusters_b:\n",
    "            links_b.update(get_long_distance_pairs(cluster))\n",
    "        \n",
    "        # Calculate correct links (intersection)\n",
    "        correct_links = links_a.intersection(links_b)\n",
    "        \n",
    "        # Calculate MUC scores\n",
    "        precision = len(correct_links) / len(links_a) if links_a else 0\n",
    "        recall = len(correct_links) / len(links_b) if links_b else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return precision, recall, f1\n",
    "\n",
    "    # Calculate both regular and MUC scores\n",
    "    muc_p, muc_r, muc_f1 = long_distance_muc(clusters_a, clusters_b)\n",
    "    \n",
    "    return {\n",
    "        'muc_scores': (muc_p, muc_r, muc_f1)\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "source": [
    "txt_path = f\"/Users/ishaan/Documents/ruihong_lab/legal_project/dataset/contacts_events_data/data/contract_events/{num}.txt\"\n",
    "ann_a = f'/Users/ishaan/Documents/ruihong_lab/legal_project/ishaan_annotations/{num}_ishaan.ann'\n",
    "ann_b = f'/Users/ishaan/Documents/ruihong_lab/legal_project/jonathan_annotations/only_long_dist/{num}_jonathan.ann'\n",
    "long_distance_evaluation(ann_a, ann_b, txt_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "source": [
    "mentions_a , corefs_a =  parse_sarc_file(ann_a)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "source": [
    "def get_long_only_clusters(annotations_file_a, annotations_file_b, text_doc):\n",
    "    # Parse both annotation files\n",
    "    mentions_a, coref_links_a = parse_sarc_file(annotations_file_a)\n",
    "    mentions_b, coref_links_b = parse_sarc_file(annotations_file_b)\n",
    "\n",
    "    # Parse paragraphs\n",
    "    with open(text_doc, 'r') as file:\n",
    "        text = file.read()\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    def get_paragraph_index(position):\n",
    "        char_count = 0\n",
    "        for idx, para in enumerate(paragraphs):\n",
    "            char_count += len(para) + 2  # +2 for '\\n\\n'\n",
    "            if position <= char_count:\n",
    "                return idx\n",
    "        return len(paragraphs) - 1\n",
    "    \n",
    "    # Filter long-distance links for both sets\n",
    "    def filter_long_distance(coref_links, mentions):\n",
    "        long_distance_links = []\n",
    "        for arg1, arg2 in coref_links:\n",
    "            pos1 = mentions[arg1][1][0]  # Get start position of first mention\n",
    "            pos2 = mentions[arg2][1][0]  # Get start position of second mention\n",
    "            \n",
    "            para1 = get_paragraph_index(pos1)\n",
    "            para2 = get_paragraph_index(pos2)\n",
    "            \n",
    "            if para1 != para2:\n",
    "                long_distance_links.append((arg1, arg2))\n",
    "                \n",
    "        return long_distance_links\n",
    "\n",
    "    coref_links_a_long = filter_long_distance(coref_links_a, mentions_a)\n",
    "    coref_links_b_long = filter_long_distance(coref_links_b, mentions_b)\n",
    "    \n",
    "    # Build clusters using filtered long-distance coreferences\n",
    "    clusters_a = build_clusters(mentions_a, coref_links_a_long)\n",
    "    clusters_b = build_clusters(mentions_b, coref_links_b_long)\n",
    "\n",
    "    return clusters_a, clusters_b"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "source": [
    "long_cluster_a, long_cluster_b = get_long_only_clusters(ann_a, ann_b, txt_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "source": [
    "b3_score(long_cluster_a, long_cluster_b)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "source": [
    "long_cluster_a"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "source": [
    "long_cluster_b"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
